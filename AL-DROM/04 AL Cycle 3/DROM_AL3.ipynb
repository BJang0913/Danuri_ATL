{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "tf.keras.optimizers.Adamax(learning_rate=1e-2)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, ZeroPadding1D\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "#from keras import objectives\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import POD_AL3 as POD_AL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = 'T'\n",
    "\n",
    "mins = [0.8,0.8,0.8,0.05,0.8,1,1,1,1,1,1,1,20,150,0.1,0.1,0.001,0.001,0.1,0.1,0.1,0.1,1,1,7,0.5,0.5,2,5,5,53,30,65,10,10,10,10,1280,0.06,0]\n",
    "maxs = [0.95,0.95,0.95,0.2,0.95,10,10,10,10,10,10,15,115,1000,1,1,0.1,0.1,1,1,1,1,5,5,12,1,1,7,10,10,58,36,70,15,15,15,15,1420,0.13,90]\n",
    "\n",
    "snap_path = 'Training_output/14OD{0}.txt'\n",
    "test_path = 'Test_output/14OV{0}.txt'\n",
    "\n",
    "n_modes = 8 # number of the PC vectors\n",
    "\n",
    "pod_p = open('POD_AL3/{0}/{1}'.format(1,scalar), 'r')\n",
    "pod_p_lines = pod_p.readlines()\n",
    "int_length = len(pod_p_lines)\n",
    "orth_basis = np.zeros((int_length,n_modes))\n",
    "\n",
    "# read PC vectors\n",
    "for i in range(n_modes):\n",
    "    pod_p = open('POD_AL3/{0}/{1}'.format(i+1,scalar), 'r')\n",
    "    pod_p_lines = pod_p.readlines()\n",
    "    pod_size = 0\n",
    "    for j in range(int_length):\n",
    "        pod_p_line = float(pod_p_lines[j])\n",
    "        pod_size = pod_size + pod_p_line*pod_p_line\n",
    "\n",
    "    for j in range(int_length):\n",
    "        pod_p_line = float(pod_p_lines[j])\n",
    "        orth_basis[j,i] =pod_p_line/math.sqrt(pod_size)\n",
    "\n",
    "# read X_unlabeled\n",
    "parameters_unlabeled = open('training_input_unlabeled.dat','r')\n",
    "parameters_lines_unlabeled = parameters_unlabeled.readlines()\n",
    "n_parameters_unlabeled = len(parameters_lines_unlabeled[0].split())\n",
    "n_snaps_unlabeled = len(parameters_lines_unlabeled)\n",
    "X_u = np.zeros((n_snaps_unlabeled,n_parameters_unlabeled))\n",
    "for k in range(n_snaps_unlabeled):\n",
    "    parameters_line_unlabeled = parameters_lines_unlabeled[k].split()\n",
    "    for kp in range(n_parameters_unlabeled):\n",
    "        X_u[k][kp]=(float(parameters_line_unlabeled[kp])-mins[kp])/(maxs[kp]-mins[kp])\n",
    "        \n",
    "\n",
    "# read input parameters\n",
    "parameters = open('training_input.dat','r')\n",
    "parameters_lines = parameters.readlines()\n",
    "n_parameters = len(parameters_lines[0].split())\n",
    "n_snaps = len(parameters_lines)\n",
    "X_p = np.zeros((n_snaps,n_parameters))\n",
    "for k in range(n_snaps):\n",
    "    parameters_line = parameters_lines[k].split()\n",
    "    for kp in range(n_parameters):\n",
    "        X_p[k][kp]=(float(parameters_line[kp])-mins[kp])/(maxs[kp]-mins[kp])\n",
    "\n",
    "# make a parameter matrix  \n",
    "X_train_full = X_p\n",
    "X = X_train_full\n",
    "X_unlabeled = X_u\n",
    "print(X.shape)\n",
    "print(X_unlabeled.shape)\n",
    "\n",
    "# make a parameter matrix for prediction\n",
    "v_parameters = open('test_input.dat','r')\n",
    "\n",
    "v_parameters_lines = v_parameters.readlines()\n",
    "n_valids = len(v_parameters_lines)\n",
    "\n",
    "X_v = np.zeros((n_valids, n_parameters))\n",
    "for k in range(n_valids):\n",
    "    v_parameters_line = v_parameters_lines[k].split()\n",
    "    for kp in range(n_parameters):\n",
    "        X_v[k][kp] = (float(v_parameters_line[kp]) - mins[kp]) / (maxs[kp] - mins[kp])\n",
    "X_test = X_v\n",
    "print(X_test.shape)\n",
    "        \n",
    "# read original matrix\n",
    "snaps = np.zeros((int_length,n_snaps))\n",
    "for k in range(n_snaps):\n",
    "    snap_p = open(snap_path.format(k+1,scalar),'r')\n",
    "    snap_p_lines = snap_p.readlines()\n",
    "\n",
    "    for j in range(int_length):\n",
    "        snap_p_line = float(snap_p_lines[j])\n",
    "        snaps[j,k] = snap_p_line        \n",
    "        \n",
    "# calculate the PC coefficients\n",
    "alpha = np.zeros((n_modes,n_snaps))\n",
    "for i in range(n_modes):\n",
    "    for k in range(n_snaps):\n",
    "        alpha[i][k] = np.inner(orth_basis[:,i],snaps[:,k])\n",
    "\n",
    "# Transposition for training DNN\n",
    "alpha = np.transpose(alpha)\n",
    "alpha_norm = np.zeros((alpha.shape))\n",
    "\n",
    "# normalize the PC coefficients\n",
    "for k in range(n_snaps):\n",
    "    for kp in range(n_modes):\n",
    "        alpha_norm[k][kp] = ( alpha[k][kp] - np.min(alpha[:,kp])) / (np.max(alpha[:,kp]) - np.min(alpha[:,kp]))\n",
    "        \n",
    "coeff_dim = n_modes \n",
    "y_train_full = alpha_norm[:100]\n",
    "y = y_train_full\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "hidden_layers_options = [2, 3, 4]  # Number of hidden layers\n",
    "neurons_options = [40, 80, 160, 320, 640, 1280] # Number of hidden neurons\n",
    "batch_size_options = [5, 10, 20]  # Batch sizes\n",
    "learning_rate_options = [0.01, 0.001, 0.0001]  # Learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers, neurons, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dense(coeff_dim))  # Output layer for regression\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the grid search, this part is commented out.\n",
    "\"\"\"\n",
    "# Grid search\n",
    "best_score = np.inf\n",
    "best_params = {}\n",
    "\n",
    "for hidden_layers in hidden_layers_options:\n",
    "    for neurons in neurons_options:\n",
    "        for batch_size in batch_size_options:\n",
    "            for learning_rate in learning_rate_options:\n",
    "                print(f\"Testing model with {hidden_layers} hidden layers, {neurons} neurons, batch size {batch_size}, and learning rate {learning_rate}\")\n",
    "                model = build_model(hidden_layers, neurons, learning_rate)\n",
    "                model.fit(X_train, y_train, batch_size=batch_size, epochs=50, verbose=0)\n",
    "                score = model.evaluate(X_val, y_val, verbose=0)\n",
    "                print(f\"Test MSE: {score}\")\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'hidden_layers': hidden_layers, 'neurons': neurons, 'batch_size': batch_size, 'learning_rate': learning_rate}\n",
    "\n",
    "print(f\"Best score (MSE): {best_score}\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model with the optimal hyperparameters\n",
    "best_params = {'hidden_layers': 2, 'neurons': 80, 'batch_size': 5, 'learning_rate': 0.01}\n",
    "\n",
    "# k-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_losses = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_cv, X_val_cv = X[train_index], X[val_index]\n",
    "    y_train_cv, y_val_cv = y[train_index], y[val_index]\n",
    "    model = build_model(best_params['hidden_layers'], best_params['neurons'], best_params['learning_rate'])\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=5, verbose=1)\n",
    "    val_loss = model.evaluate(X_val_cv, y_val_cv, verbose=0)\n",
    "    cv_losses.append(val_loss)\n",
    "\n",
    "average_cv_loss = np.mean(cv_losses)\n",
    "print(f\"Average CV Loss: {average_cv_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ENN\n",
    "X_final_train = X\n",
    "y_final_train = y\n",
    "final_models = [build_model(best_params['hidden_layers'], best_params['neurons'], best_params['learning_rate']) for _ in range(50)]\n",
    "\n",
    "for final_model in final_models:\n",
    "    history = final_model.fit(\n",
    "        X_final_train, y_final_train,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        epochs=250, # Set to a large number, early stopping will determine the actual number of epochs\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"network was trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the PC coefficients\n",
    "\n",
    "norm_p_alpha = np.mean([final_model.predict(X_test) for final_model in final_models], axis=0)\n",
    "\n",
    "p_alpha = np.zeros((norm_p_alpha.shape))\n",
    "for kp in range(len(np.transpose(alpha))):\n",
    "    p_alpha[:,kp] = norm_p_alpha[:,kp] * (np.max(alpha[:,kp]) - np.min(alpha[:,kp])) + np.min(alpha[:,kp])\n",
    "\n",
    "if n_valids == 1:\n",
    "    recons_p_field = np.matmul(basis,np.transpose(p_alpha))\n",
    "    recons_p_field = recons_p_field.flatten()\n",
    "\n",
    "else:\n",
    "    recons_p_field = np.matmul(orth_basis,np.transpose(p_alpha))\n",
    "\n",
    "sol = np.zeros((int_length, n_valids))\n",
    "\n",
    "for kp in range(n_valids):\n",
    "    for i in range(int_length):\n",
    "        sol[i, kp] = recons_p_field[i, kp]\n",
    "\n",
    "if not os.path.isdir('predict_AL3'):\n",
    "    os.mkdir('predict_AL3')\n",
    "for k in range(n_valids):\n",
    "    print('Predict {0} for parameter{1}'.format(scalar, k + 1))\n",
    "    if not os.path.isdir('predict_AL3/{0}'.format(k + 1)):\n",
    "        os.mkdir('predict_AL3/{0}'.format(k + 1))\n",
    "    reconst_p = open('predict_AL3/{0}/{1}'.format(k + 1, scalar), 'w')\n",
    "\n",
    "    if n_valids == 1:\n",
    "        sol.flatten()\n",
    "        for i in range(int_length):\n",
    "            rec_p = sol[i]\n",
    "            reconst_p.write(str(rec_p))\n",
    "            reconst_p.write('\\n')\n",
    "\n",
    "    else:\n",
    "        for i in range(int_length):\n",
    "            rec_p = sol[i, k]\n",
    "            reconst_p.write(str(rec_p))\n",
    "            reconst_p.write('\\n')\n",
    "reconst_p.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "\n",
    "n_validation = 50\n",
    "\n",
    "cfd_path = 'predict_AL3/{0}/{1}'\n",
    "val_path = 'Test_output/14OV{0}.txt'\n",
    "\n",
    "int_length = 1666 \n",
    "scalar = 'T'\n",
    "\n",
    "predict = np.zeros((int_length,n_validation))\n",
    "diff = np.zeros((int_length,n_validation))\n",
    "absol = np.zeros((int_length,n_validation))\n",
    "errsqr = np.zeros((int_length,n_validation))\n",
    "valsqr = np.zeros((int_length,n_validation))\n",
    "valsqrsum = np.zeros((n_validation))\n",
    "errsqrsum = np.zeros((n_validation))\n",
    "val_mean = np.zeros((n_validation))\n",
    "val_mean1 = np.ones((int_length, n_validation))\n",
    "sqrsum = np.zeros((int_length,n_validation))\n",
    "totsqrsum = np.zeros((n_validation))\n",
    "\n",
    "snap_stack_val = np.zeros((int_length, n_validation))\n",
    "\n",
    "for val_num in range(n_validation):\n",
    "    predict_file = open(cfd_path.format(val_num+1,scalar),'r')\n",
    "#   predict_file = open('predict/{0}/{1}'.format(val_num+1,scalar),'r')\n",
    "    predict_file_lines = predict_file.readlines()\n",
    "    snap_file = open(val_path.format(val_num+1,scalar),'r')\n",
    "    snap_file_lines = snap_file.readlines()\n",
    "    for i in range(int_length):\n",
    "        predict_file_line = float(predict_file_lines[i])\n",
    "        predict[i,val_num] = predict_file_line\n",
    "        snap_file_line = float(snap_file_lines[i])\n",
    "        snap_stack_val[i,val_num] = snap_file_line\n",
    "        \n",
    "#        diff[i,val_num] = snap_stack[i,val_num] - predict[i,val_num]\n",
    "        diff[i,val_num] = snap_stack_val[i,val_num] - predict[i,val_num]\n",
    "        absol[i,val_num] = abs(diff[i,val_num])\n",
    "        errsqr[i,val_num] = diff[i,val_num] * diff[i,val_num]\n",
    "#        valsqr[i,val_num] = snap_stack[i,val_num] * snap_stack[i,val_num]\n",
    "        valsqr[i,val_num] = snap_stack_val[i,val_num] * snap_stack_val[i,val_num]\n",
    "    errsqrsum[val_num] = np.sum(errsqr[:,val_num])\n",
    "    valsqrsum[val_num] = np.sum(valsqr[:,val_num])\n",
    "#    val_mean[val_num] = np.mean(snap_stack[:,val_num])\n",
    "    val_mean[val_num] = np.mean(snap_stack_val[:,val_num])\n",
    "    \n",
    "    val_mean1[:,val_num] = val_mean1[:,val_num] * val_mean[val_num]\n",
    "    for i in range(int_length):\n",
    "#        sqrsum[i, val_num] = (snap_stack[i,val_num] - val_mean1[i,val_num]) * (snap_stack[i,val_num] - val_mean1[i,val_num])\n",
    "        sqrsum[i, val_num] = (snap_stack_val[i,val_num] - val_mean1[i,val_num]) * (snap_stack_val[i,val_num] - val_mean1[i,val_num])\n",
    "\n",
    "    totsqrsum[val_num] = np.sum(sqrsum[:,val_num])\n",
    "\n",
    "rmse = np.zeros((1,n_validation))\n",
    "nrmse = np.zeros((1,n_validation))\n",
    "mae = np.zeros((1,n_validation))\n",
    "mape = np.zeros((1,n_validation))\n",
    "r2 = np.zeros((1,n_validation))\n",
    "l2rel = np.zeros((1,n_validation))\n",
    "\n",
    "for i in range(n_validation):\n",
    "#    rmse[0,i] = round(math.sqrt(mean_squared_error(snap_stack[:,i],predict[:,i])),8)\n",
    "    rmse[0,i] = round(math.sqrt(mean_squared_error(snap_stack_val[:,i],predict[:,i])),8)\n",
    "#    nrmse[0,i] = round(rmse[0,i]*100/(np.mean(snap_stack[:,i])),8)\n",
    "    nrmse[0,i] = round(rmse[0,i]*100/(np.mean(snap_stack_val[:,i])),8)\n",
    "    #nrmse[0,i] = round(rmse[0,i]*100/(np.max(val[:,i])-np.min(val[:,i])),8)\n",
    "#    mape[0,i] = round(100*np.sum(absol[:,i]/snap_stack[:,i])/n_mask,8)\n",
    "    mape[0,i] = round(100*np.sum(absol[:,i]/snap_stack_val[:,i])/int_length,8)\n",
    "    mae[0,i] = np.mean(absol[:,i])\n",
    "    r2[0,i] =  1 - ( errsqrsum[i] / totsqrsum[i])\n",
    "    l2rel[0,i] = math.sqrt( errsqrsum[i] / valsqrsum[i] )*100\t\n",
    "\n",
    "if not os.path.isdir('error_AL3'):\n",
    "    os.mkdir('error_AL3')\n",
    "\n",
    "err_file = open('error_AL3/error_AL3_{0}'.format(scalar),'w')\n",
    "err_file.write('RMSE')\n",
    "err_file.write('\\t\\t\\t\\t')\n",
    "err_file.write('MAE')\n",
    "err_file.write('\\t\\t\\t\\t')\n",
    "err_file.write('MAX Error')\n",
    "err_file.write('\\t\\t\\t\\t')\n",
    "err_file.write('R2')\n",
    "err_file.write('\\t\\t\\t\\t')\n",
    "err_file.write('L2 relative norm')\n",
    "err_file.write('\\n')\n",
    "\n",
    "for val_num in range(n_validation):\n",
    "    \n",
    "    print('Error check for validation {0}\\n'.format(val_num+1))\n",
    "    print('RMSE : {0}'.format(abs(rmse[0,val_num])))\n",
    "    print('MAE : {0}'.format(abs(mae[0,val_num])))\n",
    "    print('Max Error : {0}'.format(round(np.max(absol[:,val_num]),8)))\n",
    "    print('R2 : {0}'.format(r2[0,val_num]))\n",
    "    print('L2 relative norm : {0}\\n'.format(l2rel[0,val_num]))\n",
    "\n",
    "    err_file.write(str(rmse[0,val_num]))\n",
    "    err_file.write('\\t\\t\\t\\t')\n",
    "    err_file.write(str(mae[0,val_num]))\n",
    "    err_file.write('\\t\\t\\t\\t')\n",
    "    err_file.write(str(np.max(absol[:,val_num])))\n",
    "    err_file.write('\\t\\t\\t\\t')\n",
    "    err_file.write(str(r2[0,val_num]))\n",
    "    err_file.write('\\t\\t\\t\\t')\n",
    "    err_file.write(str(l2rel[0,val_num]))\n",
    "    err_file.write('\\n')\n",
    "\n",
    "print('Average RMSE  : {0}%'.format(round(np.mean(abs(nrmse[0,:])),8)))\n",
    "print('Average R2  : {0}'.format(round(np.mean(r2[0,:]),5)))\n",
    "print('Average L2 relative norm  : {0}'.format(round(np.mean(l2rel[0,:]),5)))\n",
    "err_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 4  # Number of active learning iterations\n",
    "num_samples_per_query = 20  # Number of samples to query in each iteration\n",
    "\n",
    "# Placeholder for predictions and uncertainty calculation\n",
    "predictions_unlabeled = np.array([final_model.predict(X_unlabeled) for final_model in final_models])\n",
    "print(predictions_unlabeled.shape)\n",
    "mean_preds = np.mean(predictions_unlabeled, axis=0)\n",
    "std_devs = np.std(predictions_unlabeled, axis=0)\n",
    "uncertainty = np.mean(std_devs, axis=1)\n",
    "print(mean_preds.shape)\n",
    "print(std_devs.shape)\n",
    "print(uncertainty.shape)\n",
    "\n",
    "# Select samples for labeling based on highest uncertainty\n",
    "query_indices = np.argsort(-uncertainty)[:num_samples_per_query]\n",
    "print(query_indices)\n",
    "X_queried = X_unlabeled[query_indices]\n",
    "print(X_queried.shape)\n",
    "\n",
    "X_queried_scale = np.zeros((X_queried.shape))\n",
    "for kp in range(n_parameters):\n",
    "    X_queried_scale[:,kp] = X_queried[:,kp] * (maxs[kp] - mins[kp]) + mins[kp]\n",
    "\n",
    "np.savetxt(\"Uncertainty_forAL4.txt\", uncertainty)\n",
    "np.savetxt(\"query_indices_forAL4.txt\", query_indices)\n",
    "np.savetxt(\"X_queried_forAL4.txt\", X_queried_scale)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
